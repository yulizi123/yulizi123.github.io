---
youku_id: XMjUzMTg4NzAzMg
youtube_id: CX-qVhemlSk
b_av: 16921335
b_cid: 27657251
b_page: 16
chapter: 4
title: DQN 神经网络 (Tensorflow)
publish-date: 2017-02-26
thumbnail: "/static/thumbnail-small/rl/4.2_DQN2.jpg"
description: "接着上节内容, 这节我们使用 Tensorflow
如果还不了解 Tensorflow, 这里去往
经典的 Tensorflow 视频教程
来搭建 DQN 当中的神经网络部分 (用来预测 Q 值).
"
post-headings:
  - 要点
  - 两个神经网络
  - 神经网络结构
  - 创建两个网络
---

学习资料:
  * [全部代码](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5_Deep_Q_Network){:target="_blank"}
  * [什么是 DQN 短视频]({% link _tutorials/machine-learning/ML-intro/4-06-DQN.md %})
  * 模拟视频效果[Youtube](https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_){:target="_blank"}, [Youku](http://list.youku.com/albumlist/show/id_27485743){:target="_blank"}
  * [强化学习实战]({% link _tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1.md %})
  * 论文 [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602){:target="_blank"}

{% include assign-heading.html %}

接着上节内容, 这节我们使用 Tensorflow
(如果还不了解 Tensorflow, 这里去往
[经典的 Tensorflow 视频教程]({% link _table-contents/machine-learning/tensorflow.html %}))
来搭建 DQN 当中的神经网络部分 (用来预测 Q 值).





{% include assign-heading.html %}

为了使用 Tensorflow 来实现 DQN, 比较推荐的方式是搭建两个神经网络, `target_net` 用于预测 `q_target` 值, 他不会及时更新参数.
`eval_net` 用于预测 `q_eval`, 这个神经网络拥有最新的神经网络参数. 不过这两个神经网络结构是完全一样的, 只是里面的参数不一样.
在[这个短视频里]({% link _tutorials/machine-learning/ML-intro/4-06-DQN.md %}), 能找到我们为什么要建立两个不同参数的神经网络.

{% include tut-image.html image-name="4-2-1.png" %}

{% include assign-heading.html %}

因为 DQN 的结构相比之前所讲的内容都不一样, 所以我们不使用继承来实现这次的功能.
这次我们创建一个 `DeepQNetwork` 的 class, 以及他神经网络部分的功能. 下次再说强化学习的更新部分.

```python
class DeepQNetwork:
    # 建立神经网络
    def _build_net(self):
```


{% include google-in-article-ads.html %}

{% include assign-heading.html %}

两个神经网络是为了固定住一个神经网络 (`target_net`) 的参数, `target_net` 是 `eval_net` 的一个历史版本,
拥有 `eval_net` 很久之前的一组参数, 而且这组参数被固定一段时间, 然后再被 `eval_net` 的新参数所替换.
而 `eval_net` 是不断在被提升的, 所以是一个可以被训练的网络 `trainable=True`. 而 `target_net` 的 `trainable=False`.

```python
class DeepQNetwork:
    def _build_net(self):
        # -------------- 创建 eval 神经网络, 及时提升参数 --------------
        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # 用来接收 observation
        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target') # 用来接收 q_target 的值, 这个之后会通过计算得到
        with tf.variable_scope('eval_net'):
            # c_names(collections_names) 是在更新 target_net 参数时会用到
            c_names, n_l1, w_initializer, b_initializer = \
                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, \
                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers

            # eval_net 的第一层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope('l1'):
                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)
                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)
                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)

            # eval_net 的第二层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope('l2'):
                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)
                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)
                self.q_eval = tf.matmul(l1, w2) + b2

        with tf.variable_scope('loss'): # 求误差
            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))
        with tf.variable_scope('train'):    # 梯度下降
            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)

        # ---------------- 创建 target 神经网络, 提供 target Q ---------------------
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # 接收下个 observation
        with tf.variable_scope('target_net'):
            # c_names(collections_names) 是在更新 target_net 参数时会用到
            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]

            # target_net 的第一层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope('l1'):
                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)
                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)
                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)

            # target_net 的第二层. collections 是在更新 target_net 参数时会用到
            with tf.variable_scope('l2'):
                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)
                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)
                self.q_next = tf.matmul(l1, w2) + b2

```


如果想一次性看到全部代码, 请去我的 [Github](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5_Deep_Q_Network){:target="_blank"}