---
youku_id: XMjczMzU1NzgxMg
youtube_id: 1liA7LcaKxs
b_av: 15990890
title: Asynchronous Advantage Actor-Critic (A3C)
description: "今天我们会来说说强化学习中的一种有效利用计算资源, 并且能提升训练效用的算法, Asynchronous Advantage Actor-Critic, 简称 A3C.
我们先说说没什么关系的,大家知道平行宇宙这回事. 想像现在有三个平行宇宙, 那么就意味着这3个平行宇宙上存在3个你,  而你可能在电脑前呆了很久,  对, 说的就是你! 然后你会被我催促起来做运动~  接着你 和 你 还有 你, 就无奈地在做着不同的运动,  油~ 我才不想知道你在做什么样的运动呢.  不过这3个你 都开始活动胫骨啦. 假设3个你都能互相通信, 告诉对方, “我这个动作可以有效缓解我的颈椎病”, “我做那个动作后, 腰就不痛了 “, “我活动了手臂, 肩膀就不痛了”. 这样你是不是就同时学到了对身体好的三招. 这样是不是感觉特别有效率. 让你看看更有效率的, 那就想想3个你同时在写作业, 一共3题, 每人做一题, 只用了1/3 的时间就把作业做完了. 感觉棒棒的. 哈, 你看出来了, 如果把这种方法用到强化学习, 岂不是 “牛逼lity”."
publish-date: 2017-04-28
chapter: 4
thumbnail: /static/thumbnail-small/ML-intro/A3C.png
post-headings:
  - 平行宇宙
  - 平行训练
  - 多核训练
---

学习资料:
  * [强化学习教程]({% link _table-contents/machine-learning/reinforcement-learning.html %})
  * [强化学习模拟程序](https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_){:target="_blank"}
  * A3C [Tensorflow 教程]({% link _tutorials/machine-learning/reinforcement-learning/6-3-A3C.md %})
  * A3C [Pytorch 代码](https://github.com/MorvanZhou/pytorch-A3C)
  * [强化学习实战]({% link _tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1.md %})
  * 论文 [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783.pdf){:target="_blank"}

今天我们会来说说强化学习中的一种有效利用计算资源, 并且能提升训练效用的算法, Asynchronous Advantage Actor-Critic, 简称 A3C.


**注: 本文不会涉及数学推导. 大家可以在很多其他地方找到优秀的数学推导文章.**

 {% include assign-heading.html %}

{% include tut-image.html image-name="a3c2.png" %}

我们先说说没什么关系的,大家知道平行宇宙这回事. 想像现在有三个平行宇宙, 那么就意味着这3个平行宇宙上存在3个你, 而你可能在电脑前呆了很久, 对, 说的就是你! 然后你会被我催促起来做运动~  接着你 和 你 还有 你, 就无奈地在做着不同的运动,  油~ 我才不想知道你在做什么样的运动呢.  不过这3个你 都开始活动胫骨啦. 假设3个你都能互相通信, 告诉对方, “我这个动作可以有效缓解我的颈椎病”, “我做那个动作后, 腰就不痛了 “, “我活动了手臂, 肩膀就不痛了”. 这样你是不是就同时学到了对身体好的三招. 这样是不是感觉特别有效率. 让你看看更有效率的, 那就想想3个你同时在写作业, 一共3题, 每人做一题, 只用了1/3 的时间就把作业做完了. 感觉棒棒的. 哈, 你看出来了, 如果把这种方法用到强化学习, 岂不是 “牛逼lity”.


 {% include assign-heading.html %}

{% include tut-image.html image-name="a3c3.png" %}

这就是传说中的 A3C. A3C 其实只是这种平行方式的一种而已, 它采用的是我们之前提到的
[Actor-Critic]({% link _tutorials/machine-learning/ML-intro/4-08-AC.md %}) 的形式. 为了训练一对 Actor 和 Critic,  我们将它复制多份红色的, 然后同时放在不同的平行宇宙当中, 让他们各自玩各的.  然后每个红色副本都悄悄告诉黑色的 Actor-Critic 自己在那边的世界玩得怎么样, 有哪些经验值得分享. 然后还能从黑色的 Actor-Critic 这边再次获取综合考量所有副本经验后的通关秘籍. 这样一来一回, 形成了一种有效率的强化学习方式.

{% include google-in-article-ads.html %}


{% include assign-heading.html %}

{% include tut-image.html image-name="a3c4.png" %}

我们知道目前的计算机多半是有双核, 4核, 甚至 6核, 8核. 一般的学习方法, 我们只能让机器人在一个核上面玩耍. 但是如果使用 A3C 的方法,  我们可以给他们安排去不同的核, 并行运算. 实验结果就是, 这样的计算方式往往比传统的方式快上好多倍. 那我们也多用用这样的红利吧.

