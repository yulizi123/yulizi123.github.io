---
youku_id: XMjgxNTY1NzQwOA
youtube_id: Yp29JqL_dd4
b_av: 16000257
title: "科普: 神经网络的黑盒不黑"
thumbnail: "/static/thumbnail-small/ML-intro/feature_representation.png"
publish-date: 2017-06-10
chapter: 2
description: "今天我们来说说为了理解神经网络在做什么, 对神经网络这个黑盒的正确打开方式.
当然, 这可不是人类的神经网络, 因为至今我们都还没彻底弄懂人类复杂神经网络的运行方式. 今天只来说说计算机中的人工神经网络. 我们都听说过, 神经网络是一个黑盒."
post-headings:
  - 神经网络
  - 黑盒
  - 神经网络分区
  - 举例说明
  - 迁移学习
---

今天我们来说说为了理解神经网络在做什么, 对神经网络这个黑盒的正确打开方式.

 {% include assign-heading.html %}


{% include tut-image.html image-name="feature_representation1.png" %}

当然, 这可不是人类的神经网络, 因为至今我们都还没彻底弄懂人类复杂神经网络的运行方式. 今天只来说说计算机中的人工神经网络. 我们都听说过, 神经网络是一个黑盒.

 {% include assign-heading.html %}


{% include tut-image.html image-name="feature_representation2.png" %}

呀, 咋一看, 的确挺黑的. 我们还知道, 如果你丢一个东西进这个黑盒, 他会给你丢出来另一个东西. 具体在黑盒里偷偷摸摸做了什么, 我们不得而知. 但丢出来的东西和丢进去的东西有着某些联系. 这是为什么呢? 这个黑盒里究竟又发生了什么呢?

{% include tut-image.html image-name="feature_representation3.png" %}

正好我手边有一个手电筒, 我们打开黑盒好好照亮看看. 一般来说, 神经网络是一连串神经层所组成的把输入进行加工再输出的系统. 中间的加工过程就是我们所谓的黑盒. 想把黑盒打开, 就是把神经网络给拆开. 按正常的逻辑, 我们能将神经网络分成三部分,

 {% include assign-heading.html %}


{% include tut-image.html image-name="feature_representation4.png" %}

输入端, 黑盒, 输出端. 输入端是我们能理解的物体, 一个宝宝, 输出端也是一个我们能理解的物体, 一个奶瓶. 对于神经网络, 传统的理解就是, 中间的这两层神经层在对输入信息进行加工, 好让自己的输出信息和奶瓶吻合. 但是我们如果换一个角度来想想. 此时, 我们将左边的红线移动一下

现在的输入端增加了一层, 原本我们认定为黑盒的一部分被照亮, 变成了一个已知部分. 我们将最左边两层的神经层共同看成输入端. 貌似怪怪的, 你可能会问: "可是这时的输入端不再是我们知道的”宝宝”了呀, 为什么可以这样看?" 想得没错, 它的确已经不是我们认识的宝宝啦, 但是”宝宝”这个人类定义的形象通过了一层神经网络加工, 变成了另外一种宝宝的形象,可能这种形象我们用肉眼看起来并不像宝宝, 不过计算机却能理解, 这是它所能识别的”宝宝”形象.  在专业术语中, 我们将宝宝当做特征(features), 将神经网络第一层加工后的宝宝叫做代表特征(feature representation). 如果再次移动红线, 我们的黑盒就消失了,  这时原本在黑盒里的所有神经层都被照亮. 原本的代表特征再次被加工, 变成了另一种代表特征, 同样, 再次加工形成的代表特征通常只有计算机自己看得懂, 能够理解. 所以, 与其说黑盒是在加工处理, 还不如说是在将一种代表特征转换成另一种代表特征, 一次次特征之间的转换, 也就是一次次的更有深度的理解. 比如神经网络如果接收人类手写数字的图片.

{% include google-in-article-ads.html %}

 {% include assign-heading.html %}


{% include tut-image.html image-name="feature_representation5.png" %}

然后我们将这个神经网络的输出层给拆掉, 只留下前三层, 那第3层输出的信息就是我们这些数字的3个最重要的代表特征, 换句话说, 就是用3个信息来代表整张手写数字图片的所有像素点. 我们如果把这3个信息展示出来, 我们就能很清楚的看到, 计算机是如何用3个点来代表不同的数字内容, 比如神经网络认为 1 和 0 是完全不同的, 所以他们应该被放在空间里不同的地方. 输出层就更好理解了,

{% include tut-image.html image-name="feature_representation6.png" %}

有了用3个点表示的数字代表特征, 我们就能整理整理, 将落在相同区域的数字分为一类, 如果落在了那些1所在的区域, 我们就认定张手写图片就是1, 如果是2的区域, 就认定为2. 这就是神经网络的黑盒并不黑的原因啦, 只是因为有时候代表特征太多了,我们人类没有办法看懂他们代表的是什么, 然而计算机却能看清楚它所学到的规律, 所以我们才觉得神经网络就是个黑盒. 这种代表特征的理解方式其实非常有用, 以至于人们拿着它来研究更高级的神经网络玩法. 比如迁移学习(Transfer Learning). 我们举一个例子.


 {% include assign-heading.html %}


{% include tut-image.html image-name="feature_representation7.png" %}

对于一个有分类能力的神经网络, 有时候我们只需要这套神经网络的理解能力, 并拿这种能力去处理其他问题. 所以我们保留它的代表特征转换能力. 因为有了这种能力, 就能将复杂的图片像素信息转换成更少量, 但更精辟的信息, 比如刚刚我们说将手写数字变成的3个点信息. 然后我们需要干点坏事, 将这个神经网络的输出层给拆掉. 套上另外一个神经网络, 用这种移植的方式再进行训练, 让它处理不同的问题, 比如, 预测照片里事物的价值. 现在看来, 这黑盒里开个灯, 其实还挺有用的嘛. 当你看不懂神经网络的时候, 这样想想, 是不是更好理解啦.
